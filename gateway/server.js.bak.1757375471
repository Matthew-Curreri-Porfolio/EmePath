import express from "express";
import cors from "cors";
import fetch from "node-fetch";
import { randomUUID } from "crypto";
import fs from "fs";
import path from "path";
import { fileURLToPath } from "url";
import { performance } from "node:perf_hooks";

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const LOG_DIR = path.join(__dirname, "logs");
fs.mkdirSync(LOG_DIR, { recursive: true });
const LOG_FILE = process.env.LOG_FILE || path.join(LOG_DIR, "gateway.log");

const OLLAMA = process.env.OLLAMA_URL || "http://127.0.0.1:11434";
const MODEL = process.env.MODEL || "qwen2.5-coder:7b-instruct";
const MOCK = process.env.MOCK === "1";
const TIMEOUT_MS = Number(process.env.GATEWAY_TIMEOUT_MS || 20000);
const VERBOSE = process.env.VERBOSE === "1";
const LOG_BODY = process.env.LOG_BODY === "1";

const app = express();
app.use(cors());
app.use(express.json({ limit: "4mb" }));

const stream = fs.createWriteStream(LOG_FILE, { flags: "a" });
const log = (e) => {
  const line = JSON.stringify({ ts: new Date().toISOString(), ...e });
  console.log(line);
  try { stream.write(line + "\n"); } catch {}
};

app.get("/health", (_req, res) => {
  res.json({
    ok: true,
    mock: MOCK,
    model: MODEL,
    ollama: OLLAMA,
    timeoutMs: TIMEOUT_MS,
    pid: process.pid,
  });
});

app.post("/complete", async (req, res) => {
  const id = randomUUID();
  const tStart = performance.now();

  const language = req.body?.language;
  const prefix = String(req.body?.prefix ?? "");
  const suffix = String(req.body?.suffix ?? "");
  const pathHint = req.body?.path;

  const prompt =
    `You are a code completion engine.\n` +
    `Language:${language}\n` +
    `<<<PREFIX>>>${prefix}\n` +
    `<<<SUFFIX>>>${suffix}\n` +
    `Continue between the markers with valid code only.`;

  const reqMeta = {
    id,
    event: "request_in",
    language,
    file: pathHint,
    model: MODEL,
    prefixLen: prefix.length,
    suffixLen: suffix.length,
    promptLen: prompt.length,
    mock: MOCK,
  };
  log(reqMeta);
  if (VERBOSE && LOG_BODY) {
    log({ id, event: "request_body_samples", prefixSample: prefix.slice(-120), suffixSample: suffix.slice(0, 120) });
  }

  if (MOCK) {
    const tEnd = performance.now();
    const text = "// codex: mock completion\n";
    log({ id, event: "response_out", mock: true, bytes: text.length, latencyMs: Math.round(tEnd - tStart) });
    res.type("text/plain").send(text);
    return;
  }

  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), TIMEOUT_MS);

  const upstreamUrl = `${OLLAMA}/api/generate`;
  const body = {
    model: MODEL,
    prompt,
    stream: false,
    options: { temperature: 0.2 },
  };

  log({
    id,
    event: "upstream_request",
    url: upstreamUrl,
    timeoutMs: TIMEOUT_MS,
    bodySize: JSON.stringify(body).length,
  });

  try {
    const tFetchStart = performance.now();
    const r = await fetch(upstreamUrl, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body),
      signal: controller.signal,
    });
    const tFetchEnd = performance.now();

    const status = r.status;
    const text = await r.text().catch(() => "");
    const bytes = text.length;

    let json = null;
    try { json = JSON.parse(text); } catch {}

    const respText = json?.response ?? text ?? "";
    const evalStats = {
      eval_count: json?.eval_count,
      eval_duration: json?.eval_duration,
      prompt_eval_count: json?.prompt_eval_count,
      prompt_eval_duration: json?.prompt_eval_duration,
      load_duration: json?.load_duration,
    };

    const latencyUpstreamMs = Math.round(tFetchEnd - tFetchStart);
    const latencyTotalMs = Math.round(performance.now() - tStart);

    log({
      id,
      event: "upstream_response",
      status,
      latencyUpstreamMs,
      bytes,
      ...evalStats,
    });

    if (!r.ok) {
      log({ id, event: "error", where: "upstream_not_ok", status, preview: text.slice(0, 200) });
      res.status(502).type("text/plain").send("// codex: upstream error\n");
      return;
    }

    const preview = String(respText).slice(0, 200);
    log({
      id,
      event: "response_out",
      latencyMs: latencyTotalMs,
      outBytes: String(respText).length,
      preview,
    });

    res.type("text/plain").send(String(respText));
  } catch (err) {
    const latencyTotalMs = Math.round(performance.now() - tStart);
    const reason = err?.name === "AbortError" ? "timeout" : (err?.message || "error");
    log({ id, event: "error", where: "fetch", reason, latencyMs: latencyTotalMs });
    res.status(504).type("text/plain").send("// codex: upstream timeout/error\n");
  } finally {
    clearTimeout(timeout);
  }
});

app.listen(3030, () => log({ event: "boot", msg: "gateway listening", url: "http://127.0.0.1:3030" }));
