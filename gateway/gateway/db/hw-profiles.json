{
  "machine": {
    "id": "hw_2025-09-11T01:52:05Z",
    "createdAt": "2025-09-11T01:52:05Z",
    "updatedAt": "2025-09-11T01:52:05Z",
    "hardware": {
      "type": "nvidia",
      "gpus": [
        {
          "vendor": "nvidia",
          "index": 0,
          "name": "NVIDIA GeForce RTX 4090",
          "memGiB": 24564,
          "compute": "8.9"
        }
      ]
    },
    "model": "/var/snap/ollama/common/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463",
    "fileMB": 4466,
    "layersGuess": 40,
    "threads": 31,
    "ctx": 8192,
    "batch": 2048,
    "ubatch": 1024,
    "ngl": 40,
    "env": {
      "GGML_CUDA_FORCE_MMQ": "0",
      "GGML_CUDA_FORCE_CUBLAS": "0"
    },
    "recommend": {
      "server": {
        "cmd": "./llama.cpp/build/bin/llama-server",
        "args": [
          "-m",
          "/var/snap/ollama/common/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463",
          "--host",
          "127.0.0.1",
          "--port",
          "8080",
          "--ctx-size",
          "8192",
          "--batch-size",
          "2048",
          "--ubatch-size",
          "1024",
          "-ngl",
          "40",
          "--threads",
          "31",
          "--flash-attn",
          "auto"
        ],
        "env": {
          "GGML_CUDA_FORCE_MMQ": "0",
          "GGML_CUDA_FORCE_CUBLAS": "0"
        }
      },
      "cli": {
        "cmd": "./llama.cpp/build/bin/llama-cli",
        "args": [
          "-m",
          "/var/snap/ollama/common/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463",
          "--n-predict",
          "64",
          "-ngl",
          "40",
          "--threads",
          "31",
          "--flash-attn",
          "auto"
        ],
        "env": {
          "GGML_CUDA_FORCE_MMQ": "0",
          "GGML_CUDA_FORCE_CUBLAS": "0"
        }
      },
      "finetune": {
        "cmd": "./llama.cpp/build/bin/llama-finetune",
        "args": [
          "-m",
          "/var/snap/ollama/common/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463",
          "--lora-out",
          "/tmp/adapter.gguf",
          "--lora-r",
          "16",
          "--lora-alpha",
          "16",
          "--epochs",
          "1",
          "--seq-len",
          "2048",
          "--lr",
          "1e-4"
        ],
        "env": {
          "GGML_CUDA_FORCE_MMQ": "0",
          "GGML_CUDA_FORCE_CUBLAS": "0"
        }
      },
      "m2m": {
        "serverHints": {
          "temperature": 0.2,
          "top_p": 0.8,
          "repeat_penalty": 1.05,
          "ctx_size": 8192,
          "batch": 2048,
          "ubatch": 1024
        },
        "clientDefaults": {
          "temperature": 0.2,
          "max_tokens": 256,
          "presence_penalty": 0,
          "frequency_penalty": 0
        }
      }
    },
    "scope": "machine"
  },
  "users": {}
}