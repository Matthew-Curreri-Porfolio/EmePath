import express from "express"; import cors from "cors"; import fetch from "node-fetch";
import { randomUUID } from "crypto"; import fs from "fs"; import path from "path";
import { fileURLToPath } from "url"; import { performance } from "node:perf_hooks";

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const LOG_DIR = path.join(__dirname, "logs"); fs.mkdirSync(LOG_DIR, { recursive: true });
const LOG_FILE = process.env.LOG_FILE || path.join(LOG_DIR, "gateway.log");

const OLLAMA = process.env.OLLAMA_URL || "http://127.0.0.1:11434";
const MODEL  = process.env.MODEL || "qwen2.5-coder:7b-instruct";
const MOCK   = process.env.MOCK === "1";
const TIMEOUT_MS = Number(process.env.GATEWAY_TIMEOUT_MS || 20000);
const VERBOSE = process.env.VERBOSE === "1";
const LOG_BODY = process.env.LOG_BODY === "1";

const app = express(); app.use(cors()); app.use(express.json({ limit: "4mb" }));
const stream = fs.createWriteStream(LOG_FILE, { flags: "a" });
const log = (e)=>{ const line = JSON.stringify({ ts:new Date().toISOString(), ...e }); console.log(line); try{stream.write(line+"\n");}catch{} };

app.get("/health", (_req,res)=>res.json({ ok:true, mock:MOCK, model:MODEL, ollama:OLLAMA, timeoutMs:TIMEOUT_MS, pid:process.pid }));

app.post("/complete", async (req, res) => {
  const id = randomUUID(); const t0 = performance.now();
  const language = req.body?.language; const prefix = String(req.body?.prefix ?? ""); const suffix = String(req.body?.suffix ?? ""); const file = req.body?.path;
  const prompt = `You are a code completion engine.\nLanguage:${language}\n<<<PREFIX>>>${prefix}\n<<<SUFFIX>>>${suffix}\nContinue between the markers with valid code only.`;
  log({ id, event:"request_in", language, file, model:MODEL, prefixLen:prefix.length, suffixLen:suffix.length, promptLen:prompt.length, mock:MOCK });
  if (VERBOSE && LOG_BODY) log({ id, event:"request_body_samples", prefixSample:prefix.slice(-120), suffixSample:suffix.slice(0,120) });

  if (MOCK){ const text="// codex: mock completion\n"; log({ id, event:"response_out", mock:true, bytes:text.length, latencyMs:Math.round(performance.now()-t0) }); return res.type("text/plain").send(text); }

  const controller = new AbortController(); const to = setTimeout(()=>controller.abort(), TIMEOUT_MS);
  const body = { model:MODEL, prompt, stream:false, options:{ temperature:0.2 } };
  log({ id, event:"upstream_request", url:`${OLLAMA}/api/generate`, timeoutMs:TIMEOUT_MS, bodySize:JSON.stringify(body).length });

  try{
    const t1=performance.now();
    const r = await fetch(`${OLLAMA}/api/generate`, { method:"POST", headers:{ "Content-Type":"application/json" }, body:JSON.stringify(body), signal:controller.signal });
    const t2=performance.now(); const status=r.status; const raw=await r.text().catch(()=> ""); let json=null; try{ json=JSON.parse(raw) }catch{}
    const resp = String(json?.response ?? raw ?? ""); const latencyUp=Math.round(t2-t1); const latencyAll=Math.round(performance.now()-t0);
    log({ id, event:"upstream_response", status, latencyUpstreamMs:latencyUp, bytes:resp.length, eval_count:json?.eval_count, eval_duration:json?.eval_duration, prompt_eval_count:json?.prompt_eval_count, prompt_eval_duration:json?.prompt_eval_duration, load_duration:json?.load_duration });
    if(!r.ok){ log({ id, event:"error", where:"upstream_not_ok", status, preview:raw.slice(0,200) }); return res.status(502).type("text/plain").send("// codex: upstream error\n"); }
    log({ id, event:"response_out", latencyMs:latencyAll, outBytes:resp.length, preview:resp.slice(0,200) });
    return res.type("text/plain").send(resp);
  }catch(e){
    const latencyAll=Math.round(performance.now()-t0); const reason = e?.name==="AbortError"?"timeout":(e?.message||"error");
    log({ id, event:"error", where:"fetch", reason, latencyMs:latencyAll }); return res.status(504).type("text/plain").send("// codex: upstream timeout/error\n");
  }finally{ clearTimeout(to); }
});

app.listen(3030, ()=>log({ event:"boot", msg:"gateway listening", url:"http://127.0.0.1:3030" }));
