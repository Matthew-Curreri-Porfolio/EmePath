# Explanation — Why Adapters (LoRA/QLoRA)

Adapters add low-rank updates **ΔW** to frozen base weights, yielding specialization without duplicating parameters.  
Benefits: small artifacts, fast iteration, safer rollbacks.

