# How-to â€” Fine-tune with LoRA/QLoRA

**Outcome:** Produce a LoRA adapter without altering base weights.

- Prepare dataset (license, PII scrub, splits)  
- Configure LoRA (r, alpha, dropout, target modules)  
- Train, checkpoint, evaluate; export adapter  
- Register adapter metadata in `/docs/models/MODEL_CARD_*.md`

